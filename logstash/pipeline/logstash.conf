input {
    file {
        path => ["/usr/share/logstash/input/logs/*"]
        start_position => "beginning"
        sincedb_path => "/dev/null"
        add_field => {"[projectid]" => "test_logs"}
    }
    file {
        path => ["/usr/share/logstash/input/csv/*"]
        start_position => "beginning"
        sincedb_path => "/dev/null"
        add_field => {"[projectid]" => "test_datasets"}
    }
}
filter {
    if [projectid] == "test_logs" {
        grok {
            match => { "message" => "%{COMBINEDAPACHELOG}"}
        }
        date {
            match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
        }
    }
    if [projectid] == "test_datasets" {
        csv {
            #autodetect_column_names => "true"

            # autodetect only works when you have single pipeline worker
            # by default, their number is set by logstash based on your CPU
            # but it can be set in logstash.yml

            skip_header => "true"
            columns => ["gender","race/ethnicity","parental level of education","lunch","test preparation course","math score","reading score","writing score"]
        }
    }
}
output {
    if [projectid] == "test_logs" {
        elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "logs"
        }
    }
    if [projectid] == "test_datasets" {
        elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "datasets"
        }
    }
}